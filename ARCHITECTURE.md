# ğŸ—ï¸ News Digest Agent - Architecture Guide for Beginners

## ğŸ¯ **What Does This System Do?**

Think of your News Digest Agent as a **personal news assistant** that:
1. **Collects** news from multiple sources (like BBC, CNN, Reddit)
2. **Summarizes** each article using AI (your local LLM)
3. **Organizes** stories by category (Ukraine ğŸ‡ºğŸ‡¦, AI ğŸ¤–, Finance ğŸ’°, etc.)
4. **Delivers** a beautiful digest to your web browser
5. **Repeats** this process automatically every hour

---

## ğŸ­ **High-Level Architecture**

```
ğŸ“° News Sources â†’ ğŸ”„ Collection â†’ ğŸ¤– AI Summary â†’ ğŸ—„ï¸ Database â†’ ğŸŒ Web Interface
     (RSS)           (Python)        (LM Studio)     (PostgreSQL)    (FastAPI/HTML)
```

**Simple explanation:**
- **Left side**: Gets raw news from the internet
- **Middle**: Your Mac Mini processes and improves it
- **Right side**: Shows you the final polished result

---

## ğŸ”§ **Key Components Explained**

### 1. **Docker Containers** (The "Boxes" That Run Everything)

```
ğŸ³ Your Mac Mini runs 4 containers:
â”œâ”€â”€ ğŸ“± app        â†’ Your main news processing application
â”œâ”€â”€ ğŸ—„ï¸ db         â†’ PostgreSQL database (stores articles & digests)
â”œâ”€â”€ ğŸ”„ redis      â†’ Redis (handles real-time message passing)
â””â”€â”€ ğŸ› ï¸ redis-commander â†’ Web interface to peek inside Redis
```

**Think of containers like:** Separate apps running on your computer, but isolated from each other.

### 2. **The App Container** (Where the Magic Happens)

```
ğŸ“± app/
â”œâ”€â”€ ğŸ•·ï¸ scrapers/     â†’ Gets news from websites
â”œâ”€â”€ ğŸ¤– pipeline/     â†’ Processes articles with AI
â”œâ”€â”€ â° scheduler/    â†’ Runs tasks automatically  
â”œâ”€â”€ ğŸŒ templates/    â†’ HTML for the web interface
â””â”€â”€ ğŸ› ï¸ main.py       â†’ The central control system
```

---

## ğŸ”„ **Data Flow Journey** (How News Becomes Your Digest)

### **Step 1: Collection** (Every Hour)
```
ğŸŒ Internet â†’ ğŸ•·ï¸ Scrapers â†’ ğŸ“Š Raw Articles
```
- **Scrapers** visit news websites (BBC, CNN, etc.)
- Download RSS feeds (structured news data)
- Extract article titles, content, and URLs

### **Step 2: Streaming** (Real-Time Processing)
```
ğŸ“Š Raw Articles â†’ ğŸ”„ Redis Streams â†’ ğŸ“ Processing Queue
```
- **Redis Streams** = like a conveyor belt for articles
- Each article gets added to a queue for processing
- Multiple articles can be processed simultaneously

### **Step 3: AI Enhancement** (The Smart Part)
```
ğŸ“ Long Article â†’ ğŸ¤– LM Studio â†’ âœ¨ Short Summary
```
- **LM Studio** (running on your Mac) reads each article
- Creates 1-2 sentence summaries
- Keeps the key facts, removes fluff

### **Step 4: Organization** (Making Sense of Everything)
```
âœ¨ Summaries â†’ ğŸ·ï¸ Categories â†’ ğŸ“‹ Digest
```
- **Categorizer** sorts articles by topic
- Ukraine news â†’ ğŸ‡ºğŸ‡¦, AI news â†’ ğŸ¤–, etc.
- Creates a digest with top stories from each category

### **Step 5: Delivery** (What You See)
```
ğŸ“‹ Digest â†’ ğŸŒ Web Interface â†’ ğŸ‘€ Your Browser
```
- **FastAPI** serves the web page
- **HTML templates** make it look beautiful
- **Auto-refresh** keeps it current

---

## ğŸ—‚ï¸ **Code Organization** (What Each File Does)

### **ğŸ•·ï¸ Scrapers** (`app/scrapers/`)
**What they do:** Fetch news from different sources

```python
# Example: app/scrapers/mainstream.py
class MainstreamScraper:
    async def scrape(self):
        # Visit BBC, CNN, Guardian RSS feeds
        # Extract article data
        # Return list of articles
```

**Key files:**
- `base.py` â†’ Common scraping functions
- `mainstream.py` â†’ BBC, CNN, Guardian, FT
- `tech.py` â†’ Ars Technica, The Verge
- `swiss.py` â†’ NZZ, Tagesanzeiger
- `social.py` â†’ Reddit (Twitter planned)

### **ğŸ¤– Pipeline** (`app/pipeline/`)
**What it does:** Processes articles with AI

```python
# Example: app/pipeline/summarizer.py
class LMStudioSummarizer:
    async def summarize_article(self, article):
        # Send article to your local LLM
        # Get back a short summary
        # Return clean, factual summary
```

**Key files:**
- `streams.py` â†’ Redis message handling
- `summarizer.py` â†’ AI summarization with LM Studio

### **â° Scheduler** (`app/scheduler/`)
**What it does:** Runs tasks automatically

```python
# Example: app/scheduler/tasks.py
scheduler.add_job(
    collect_news,           # What to do
    CronTrigger(minute=0),  # When: every hour at :00
    id="hourly_collection"  # Name for this job
)
```

**Schedule:**
- **Every hour**: Collect and process news
- **6:00 AM**: Create morning digest
- **10:00 PM**: Create evening digest
- **11 PM - 6 AM**: Quiet hours

### **ğŸŒ Web Interface** (`app/main.py` + `templates/`)
**What it does:** Shows you the results

```python
# Example: app/main.py
@app.get("/")  # When someone visits your homepage
async def read_digest():
    # Get latest digest from database
    # Format it nicely
    # Show it as HTML webpage
```

**Templates:**
- `base.html` â†’ Common page structure
- `digest.html` â†’ Main news digest page  
- `archive.html` â†’ Past digests

---

## ğŸ—„ï¸ **Database Design** (How Data Is Stored)

### **Articles Table**
```
ğŸ“„ Each news article has:
â”œâ”€â”€ ğŸ”— url           â†’ Link to original article
â”œâ”€â”€ ğŸ“° title         â†’ Headline
â”œâ”€â”€ ğŸ“ content       â†’ Full article text
â”œâ”€â”€ âœ¨ summary       â†’ AI-generated summary
â”œâ”€â”€ ğŸ·ï¸ category     â†’ ukraine, ai_data, finance, etc.
â”œâ”€â”€ ğŸ“Š engagement   â†’ How popular/important
â””â”€â”€ â° timestamps   â†’ When published/scraped
```

### **Digests Table**
```
ğŸ“‹ Each digest contains:
â”œâ”€â”€ ğŸ†” id            â†’ Unique identifier
â”œâ”€â”€ ğŸ“… created_at    â†’ When digest was made
â”œâ”€â”€ ğŸ• digest_type   â†’ hourly, morning, evening
â”œâ”€â”€ ğŸ“Š stories_count â†’ Number of articles
â””â”€â”€ ğŸ·ï¸ categories   â†’ List of topics covered
```

### **Links Table** (`DigestArticle`)
```
ğŸ”— Connects digests to articles:
â”œâ”€â”€ ğŸ“‹ digest_id     â†’ Which digest
â”œâ”€â”€ ğŸ“„ article_id    â†’ Which article  
â”œâ”€â”€ ğŸ“ position      â†’ Order in digest
â””â”€â”€ ğŸ·ï¸ category_group â†’ Topic grouping
```

---

## ğŸ› ï¸ **Technologies Used** (And Why)

### **Languages & Frameworks**
- **ğŸ Python**: Main language (easy to read, great for AI/web)
- **âš¡ FastAPI**: Web framework (fast, modern, auto-documentation)
- **ğŸ”„ AsyncIO**: Handles multiple tasks simultaneously
- **ğŸ“„ Jinja2**: Template engine (turns data into HTML)

### **Databases & Storage**
- **ğŸ˜ PostgreSQL**: Main database (reliable, powerful)
- **ğŸ”„ Redis**: Message queue & caching (super fast)
- **ğŸ“Š SQLModel**: Database models (type-safe, modern)

### **AI & Processing**
- **ğŸ¤– LM Studio**: Local AI (runs on your Mac, private)
- **ğŸ•·ï¸ Beautiful Soup**: HTML parsing (cleans up web content)
- **ğŸ“° feedparser**: RSS feed processing (standardized news format)

### **Infrastructure**
- **ğŸ³ Docker**: Containerization (everything runs consistently)
- **â˜ï¸ Cloudflare**: Global CDN & tunneling (fast, secure access)
- **â° APScheduler**: Task scheduling (automatic timing)

---

## ğŸ” **How to Explore the Code**

### **Start with the main flow:**
1. `app/main.py` â†’ See the web interface
2. `app/scheduler/tasks.py` â†’ See the automation
3. `app/scrapers/mainstream.py` â†’ See news collection
4. `app/pipeline/summarizer.py` â†’ See AI integration

### **Key functions to understand:**
```python
# In main.py
read_digest()      # Shows you the main page

# In tasks.py  
collect_news()     # Gathers articles hourly
create_digest()    # Makes the final digest

# In summarizer.py
summarize_article() # Uses AI to create summaries
```

### **Configuration:**
- `app/config.py` â†’ All settings (news sources, timing, etc.)
- `.env` â†’ Your personal settings (LM Studio connection)
- `docker-compose.yml` â†’ How services connect

---

## ğŸ¯ **Key Concepts for Beginners**

### **Async Programming**
```python
async def fetch_news():  # Can do other things while waiting
    data = await get_rss_feed()  # Wait for this to finish
    return data
```
**Why:** Your system can process multiple articles simultaneously instead of one-by-one.

### **Database Models**
```python
class Article(SQLModel, table=True):  # This creates a database table
    title: str                        # Article headline
    summary: Optional[str] = None     # AI summary (might be empty)
```
**Why:** Ensures data is stored consistently and safely.

### **Web APIs**
```python
@app.get("/health")              # When someone visits /health
async def health_check():        # Run this function
    return {"status": "healthy"} # Send back this data
```
**Why:** Other programs (or you) can check if the system is working.

### **Containerization**
```yaml
services:
  app:          # Your main application
    build: .    # Build from local code
    ports:      # Make port 8000 accessible
      - "8000:8000"
```
**Why:** Everything runs the same way on any computer.

---

## ğŸš€ **What Happens When You Start It**

1. **ğŸ³ Docker starts 4 containers**
2. **ğŸ—„ï¸ Database creates tables** for articles and digests
3. **ğŸ”„ Redis starts the message queue**
4. **â° Scheduler begins hourly tasks**
5. **ğŸŒ Web server starts listening** on port 8000
6. **ğŸ•·ï¸ First news collection begins** within the hour
7. **ğŸ¤– LM Studio starts summarizing** articles
8. **ğŸ“‹ Your first digest appears** at http://localhost:8000

---

## ğŸ‰ **You've Built Something Amazing!**

Your News Digest Agent is a **sophisticated AI-powered system** that:
- âœ… **Scales**: Can handle hundreds of articles per hour
- âœ… **Intelligent**: Uses AI to understand and summarize content  
- âœ… **Reliable**: Runs 24/7 with error handling and recovery
- âœ… **Private**: All AI processing happens on your Mac
- âœ… **Global**: Accessible from anywhere via Cloudflare
- âœ… **Customizable**: Easy to add new sources or change behavior

You're now running **enterprise-grade news intelligence** from your home! ğŸ ğŸ¤–ğŸ“°
